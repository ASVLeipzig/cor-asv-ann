{
  "git_url": "https://github.com/ASVLeipzig/cor-asv-ann",
  "version": "0.1.15",
  "tools": {
    "ocrd-cor-asv-ann-process": {
      "executable": "ocrd-cor-asv-ann-process",
      "categories": [
        "Text recognition and optimization"
      ],
      "steps": [
        "recognition/post-correction"
      ],
      "description": "Improve text annotation by character-level encoder-attention-decoder ANN model",
      "input_file_grp": [
        "OCR-D-OCR-TESS",
        "OCR-D-OCR-KRAK",
        "OCR-D-OCR-OCRO",
        "OCR-D-OCR-CALA",
        "OCR-D-OCR-ANY"
      ],
      "output_file_grp": [
        "OCR-D-COR-ASV"
      ],
      "parameters": {
        "model_file": {
          "type": "string",
          "format": "uri",
          "content-type": "application/x-hdf;subtype=bag",
          "description": "path of h5py weight/config file for model trained with cor-asv-ann-train",
          "required": true,
          "cacheable": true
        },
        "textequiv_level": {
          "type": "string",
          "enum": ["line", "word", "glyph"],
          "default": "glyph",
          "description": "PAGE XML hierarchy level to read/write TextEquiv input/output on"
        },
        "charmap": {
          "type": "object",
          "default": {},
          "description": "mapping for input characters before passing to correction; can be used to adapt to character set mismatch between input and model (without relying on underspecification alone)"
        },
        "rejection_threshold": {
          "type": "number",
          "format": "float",
          "default": 0.5,
          "description": "minimum probability of the candidate corresponding to the input character in each hypothesis during beam search, helps balance precision/recall trade-off; set to 0 to disable rejection (max recall) or 1 to disable correction (max precision)"
        },
        "relative_beam_width": {
          "type": "number",
          "format": "float",
          "default": 0.2,
          "description": "minimum fraction of the best candidate's probability required to enter the beam in each hypothesis; controls the quality/performance trade-off"
        },
        "fixed_beam_width": {
          "type": "number",
          "format": "integer",
          "default": 15,
          "description": "maximum number of candidates allowed to enter the beam in each hypothesis; controls the quality/performance trade-off"
        },
        "fast_mode": {
          "type": "boolean",
          "default": false,
          "description": "decode greedy instead of beamed, with batches of parallel lines instead of parallel alternatives; also disables rejection and beam parameters; enable if performance is far more important than quality"
        }
      }
    },
    "ocrd-cor-asv-ann-evaluate": {
      "executable": "ocrd-cor-asv-ann-evaluate",
      "categories": [
        "Text recognition and optimization"
      ],
      "steps": [
        "recognition/evaluation"
      ],
      "description": "Align different textline annotations and compute distance",
      "input_file_grp": [
        "OCR-D-GT-SEG-LINE",
        "OCR-D-OCR-TESS",
        "OCR-D-OCR-KRAK",
        "OCR-D-OCR-OCRO",
        "OCR-D-OCR-CALA",
        "OCR-D-OCR-ANY",
        "OCR-D-COR-ASV"
      ],
      "output_file_grp": [
        "OCR-D-EVAL-CER"
      ],
      "parameters": {
        "match_on": {
          "type": "string",
          "enum": ["index", "id", "coords", "baseline"],
          "default": "id",
          "description": "Attribute to differentiate input annotations by: either `TextEquiv/@index` of the same TextLine and input file, or `TextLine/@id` (or `./Coords/@points` or `./Baseline/@points`) of input files across input fileGrps."
        },
        "metric": {
          "type": "string",
          "enum": ["Levenshtein-fast", "Levenshtein", "NFC", "NFKC", "historic_latin"],
          "default": "Levenshtein-fast",
          "description": "Distance metric to calculate and aggregate: `historic_latin` for GT level 1-3, `NFKC` for roughly GT level 2 (but including reduction of `ſ/s` and superscript numerals etc), `Levenshtein` for GT level 3 (or `Levenshtein-fast` for faster alignment - but using maximum sequence length instead of path length as CER denominator, and without confusion statistics)."
        },
        "gt_level": {
          "type": "number",
          "enum": [1, 2, 3],
          "default": 1,
          "description": "When `metric=historic_latin`, normalize and equate at this GT transcription level."
        },
        "confusion": {
          "type": "number",
          "format": "integer",
          "minimum": 0,
          "default": 0,
          "description": "Count edits and show that number of most frequent confusions (non-identity) in the end."
        },
        "histogram": {
          "type": "boolean",
          "default": false,
          "description": "Aggregate and show mutual character histograms."
        }
      }
    },
    "ocrd-cor-asv-ann-align": {
      "executable": "ocrd-cor-asv-ann-align",
      "categories": [
        "Text recognition and optimization"
      ],
      "steps": [
        "recognition/post-correction"
      ],
      "description": "Align different textline annotations and pick best",
      "input_file_grp": [
        "OCR-D-GT-SEG-LINE",
        "OCR-D-OCR-TESS",
        "OCR-D-OCR-KRAK",
        "OCR-D-OCR-OCRO",
        "OCR-D-OCR-CALA",
        "OCR-D-OCR-ANY",
        "OCR-D-COR-ASV"
      ],
      "output_file_grp": [
        "OCR-D-OCR-MULTI"
      ],
      "parameters": {
        "method": {
          "type": "string",
          "enum": ["majority", "confidence", "combined"],
          "default": "majority",
          "description": "decide by majority of OCR hypotheses, by highest confidence of OCRs or by a combination thereof"
        }
      }
    },
    "ocrd-cor-asv-ann-join": {
      "executable": "ocrd-cor-asv-ann-join",
      "categories": [
        "Text recognition and optimization"
      ],
      "steps": [
        "recognition/post-correction"
      ],
      "description": "Join different textline annotations by concatenation",
      "input_file_grp": [
        "OCR-D-GT-SEG-LINE",
        "OCR-D-OCR-TESS",
        "OCR-D-OCR-KRAK",
        "OCR-D-OCR-OCRO",
        "OCR-D-OCR-CALA",
        "OCR-D-OCR-ANY",
        "OCR-D-COR-ASV"
      ],
      "output_file_grp": [
        "OCR-D-OCR-MULTI"
      ],
      "parameters": {
        "add-filegrp-comments": {
          "type": "boolean",
          "default": false,
          "description": "set @comments of each TextEquiv to the fileGrp/@USE it came from"
        },
        "add-filegrp-index": {
          "type": "boolean",
          "default": false,
          "description": "set @index of each TextEquiv to the fileGrp index (zero based) it came from"
        },
        "match-on": {
          "type": "string",
          "enum": ["id", "coords", "baseline"],
          "default": "id",
          "description": "information to match lines on (element @id, Coords/@points, Baseline/@points)"
        }
      }
    },
    "ocrd-cor-asv-ann-mark": {
      "executable": "ocrd-cor-asv-ann-mark",
      "description": "mark words not found by a spellchecker",
      "steps": ["recognition/post-correction"],
      "categories": ["Text recognition and optimization"],
      "parameters": {
        "command": {
          "type": "string",
          "required": true,
          "description": "external tool to query word forms, e.g. 'hunspell -i utf-8 -d de_DE,en_US -w'"
        },
        "normalization": {
          "type": "object",
          "default": {},
          "description": "mapping of characters prior to spellcheck, e.g. {'ſ': 's', 'aͤ': 'ä'}"
        },
        "format": {
          "type": "string",
          "default": "conf",
          "description": "how unknown words should be marked; if 'conf', then writes @conf=0.123, otherwise writes that value into @comments"
        }
      }
    }
  }
}
